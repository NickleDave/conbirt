
@article{wirthlin_modular_2019,
	title = {A {Modular} {Approach} to {Vocal} {Learning}: {Disentangling} the {Diversity} of a {Complex} {Behavioral} {Trait}},
	volume = {104},
	issn = {08966273},
	shorttitle = {A {Modular} {Approach} to {Vocal} {Learning}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319308396},
	doi = {10.1016/j.neuron.2019.09.036},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {Neuron},
	author = {Wirthlin, Morgan and Chang, Edward F. and Knörnschild, Mirjam and Krubitzer, Leah A. and Mello, Claudio V. and Miller, Cory T. and Pfenning, Andreas R. and Vernes, Sonja C. and Tchernichovski, Ofer and Yartsev, Michael M.},
	month = oct,
	year = {2019},
	pages = {87--99},
	file = {Wirthlin et al. - 2019 - A Modular Approach to Vocal Learning Disentanglin.pdf:/home/pimienta/Zotero/storage/L7JW7EGY/Wirthlin et al. - 2019 - A Modular Approach to Vocal Learning Disentanglin.pdf:application/pdf},
}

@article{sainburg_toward_2021,
	title = {Toward a {Computational} {Neuroethology} of {Vocal} {Communication}: {From} {Bioacoustics} to {Neurophysiology}, {Emerging} {Tools} and {Future} {Directions}},
	volume = {15},
	issn = {1662-5153},
	shorttitle = {Toward a {Computational} {Neuroethology} of {Vocal} {Communication}},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2021.811737/full},
	doi = {10.3389/fnbeh.2021.811737},
	abstract = {Recently developed methods in computational neuroethology have enabled increasingly detailed and comprehensive quantiﬁcation of animal movements and behavioral kinematics. Vocal communication behavior is well poised for application of similar large-scale quantiﬁcation methods in the service of physiological and ethological studies. This review describes emerging techniques that can be applied to acoustic and vocal communication signals with the goal of enabling study beyond a small number of model species. We review a range of modern computational methods for bioacoustics, signal processing, and brain-behavior mapping. Along with a discussion of recent advances and techniques, we include challenges and broader goals in establishing a framework for the computational neuroethology of vocal communication.},
	language = {en},
	urldate = {2022-01-22},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Sainburg, Tim and Gentner, Timothy Q.},
	month = dec,
	year = {2021},
	pages = {811737},
	file = {Sainburg and Gentner - 2021 - Toward a Computational Neuroethology of Vocal Comm.pdf:/home/pimienta/Zotero/storage/AZFQEZ85/Sainburg and Gentner - 2021 - Toward a Computational Neuroethology of Vocal Comm.pdf:application/pdf},
}

@article{kershenbaum_acoustic_2016,
	title = {Acoustic sequences in non-human animals: a tutorial review and prospectus: {Acoustic} sequences in animals},
	volume = {91},
	issn = {14647931},
	shorttitle = {Acoustic sequences in non-human animals},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/brv.12160},
	doi = {10.1111/brv.12160},
	abstract = {Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise – let alone understand – the signiﬁcance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be deﬁned and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the ﬁelds of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, ‘Analysing vocal sequences in animals’. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this ﬁeld, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different ﬁelds, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.},
	language = {en},
	number = {1},
	urldate = {2021-11-22},
	journal = {Biological Reviews},
	author = {Kershenbaum, Arik and Blumstein, Daniel T. and Roch, Marie A. and Akçay, Çağlar and Backus, Gregory and Bee, Mark A. and Bohn, Kirsten and Cao, Yan and Carter, Gerald and Cäsar, Cristiane and Coen, Michael and DeRuiter, Stacy L. and Doyle, Laurance and Edelman, Shimon and Ferrer-i-Cancho, Ramon and Freeberg, Todd M. and Garland, Ellen C. and Gustison, Morgan and Harley, Heidi E. and Huetz, Chloé and Hughes, Melissa and Hyland Bruno, Julia and Ilany, Amiyaal and Jin, Dezhe Z. and Johnson, Michael and Ju, Chenghui and Karnowski, Jeremy and Lohr, Bernard and Manser, Marta B. and McCowan, Brenda and Mercado, Eduardo and Narins, Peter M. and Piel, Alex and Rice, Megan and Salmi, Roberta and Sasahara, Kazutoshi and Sayigh, Laela and Shiu, Yu and Taylor, Charles and Vallejo, Edgar E. and Waller, Sara and Zamora-Gutierrez, Veronica},
	month = feb,
	year = {2016},
	pages = {13--52},
	file = {Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf:/home/pimienta/Zotero/storage/ABH3RYWI/Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf:application/pdf},
}

@article{stowell_computational_2022,
	title = {Computational bioacoustics with deep learning: a review and roadmap},
	abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider ﬁeld of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
	language = {en},
	author = {Stowell, Dan},
	year = {2022},
	pages = {46},
	file = {Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf:/home/pimienta/Zotero/storage/PAAGY2CU/Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf:application/pdf},
}

@article{hauser_faculty_2002,
	title = {The {Faculty} of {Language}: {What} {Is} {It}, {Who} {Has} {It}, and {How} {Did} {It} {Evolve}?},
	volume = {298},
	issn = {0036-8075, 1095-9203},
	shorttitle = {The {Faculty} of {Language}},
	url = {https://www.science.org/doi/10.1126/science.298.5598.1569},
	doi = {10.1126/science.298.5598.1569},
	abstract = {We argue that an understanding of the faculty of language requires substantial interdisciplinary cooperation. We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN). FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language. We further argue that FLN may have evolved for reasons other than language, hence comparative studies might look for evidence of such computations outside of the domain of communication (for example, number, navigation, and social relations).},
	language = {en},
	number = {5598},
	urldate = {2022-07-06},
	journal = {Science},
	author = {Hauser, Marc D. and Chomsky, Noam and Fitch, W. Tecumseh},
	month = nov,
	year = {2002},
	pages = {1569--1579},
	file = {Hauser et al. - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf:/home/pimienta/Zotero/storage/HY959NW7/Hauser et al. - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf:application/pdf},
}

@phdthesis{fukuzawa_computational_2022,
	type = {{PhD} {Thesis}},
	title = {Computational methods for a generalised acoustics analysis workflow: a thesis presented in partial fulfilment of the requirements for the degree of {Master} of {Science} in {Computer} {Science} at {Massey} {University}, {Auckland}, {New} {Zealand}},
	shorttitle = {Computational methods for a generalised acoustics analysis workflow},
	school = {Massey University},
	author = {Fukuzawa, Yukio},
	year = {2022},
	file = {FukuzawaMPhilThesis.pdf:/home/pimienta/Zotero/storage/8PCXKJU5/FukuzawaMPhilThesis.pdf:application/pdf;Full Text:/home/pimienta/Zotero/storage/C6L86Y4D/Fukuzawa - 2022 - Computational methods for a generalised acoustics .pdf:application/pdf;Snapshot:/home/pimienta/Zotero/storage/TX268GES/17296.html:text/html},
}

@article{goffinet_low-dimensional_2021,
	title = {Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires},
	volume = {10},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/67855},
	doi = {10.7554/eLife.67855},
	abstract = {Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-­specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-­by-m­ oment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-­dimensional vocal behavior.},
	language = {en},
	urldate = {2022-08-09},
	journal = {eLife},
	author = {Goffinet, Jack and Brudner, Samuel and Mooney, Richard and Pearson, John},
	month = may,
	year = {2021},
	pages = {e67855},
	file = {Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf:/home/pimienta/Zotero/storage/8KFUVGLW/Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf:application/pdf},
}

@misc{paul_boersma_praat_2021,
	title = {Praat: doing phonetics by computer},
	url = {http://www.praat.org/},
	doi = {10.1097/aud.0b013e31821473f7},
	author = {{Paul Boersma} and {David Weenink}},
	year = {2021},
}

@misc{audacity_team_audacity_2019,
	title = {Audacity},
	url = {https://www.audacityteam.org/},
	abstract = {Audacity® software is copyright © 1999-2019 Audacity Team.
Web site: https://audacityteam.org/. It is free software
distributed under the terms of the GNU General Public License.
The name Audacity® is a registered trademark of Dominic Mazzoni.},
	urldate = {2020-02-22},
	author = {{Audacity Team}},
	year = {2019},
}

@misc{program_raven_2016,
	title = {Raven {Lite}: {Interactive} {Sound} {Analysis} {Software} ({Version} 2.0)},
	publisher = {The Cornell Lab of Ornithology Ithaca, NY},
	author = {Program, Bioacoustics Research},
	year = {2016},
}

@article{charif_raven_2006,
	title = {Raven {Lite} 1.0 user’s guide},
	journal = {Cornell Laboratory of Ornithology, Ithaca, NY},
	author = {Charif, RA and Ponirakis, DW and Krein, TP},
	year = {2006},
}

@article{mcgregor_shared_2022,
	title = {Shared mechanisms of auditory and non-auditory vocal learning in the songbird brain},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.75691},
	doi = {10.7554/eLife.75691},
	abstract = {Songbirds and humans share the ability to adaptively modify their vocalizations based on sensory feedback. Prior studies have focused primarily on the role that auditory feedback plays in shaping vocal output throughout life. In contrast, it is unclear how non-auditory information drives vocal plasticity. Here, we first used a reinforcement learning paradigm to establish that somatosensory feedback (cutaneous electrical stimulation) can drive vocal learning in adult songbirds. We then assessed the role of a songbird basal ganglia thalamocortical pathway critical to auditory vocal learning in this novel form of vocal plasticity. We found that both this circuit and its dopaminergic inputs are necessary for non-auditory vocal learning, demonstrating that this pathway is critical for guiding adaptive vocal changes based on both auditory and somatosensory signals. The ability of this circuit to use both auditory and somatosensory information to guide vocal learning may reflect a general principle for the neural systems that support vocal plasticity across species.},
	urldate = {2023-03-09},
	journal = {eLife},
	author = {McGregor, James N and Grassler, Abigail L and Jaffe, Paul I and Jacob, Amanda Louise and Brainard, Michael S and Sober, Samuel J},
	editor = {Goldberg, Jesse H and Shinn-Cunningham, Barbara G and Giret, Nicolas},
	month = sep,
	year = {2022},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {songbird, Bengalese finch, lonchura striata var. domestica},
	pages = {e75691},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/VMYTLYGD/McGregor et al. - 2022 - Shared mechanisms of auditory and non-auditory voc.pdf:application/pdf},
}

@article{provost_impacts_2022,
	title = {The impacts of fine-tuning, phylogenetic distance, and sample size on big-data bioacoustics},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0278522},
	doi = {10.1371/journal.pone.0278522},
	abstract = {Vocalizations in animals, particularly birds, are critically important behaviors that influence their reproductive fitness. While recordings of bioacoustic data have been captured and stored in collections for decades, the automated extraction of data from these recordings has only recently been facilitated by artificial intelligence methods. These have yet to be evaluated with respect to accuracy of different automation strategies and features. Here, we use a recently published machine learning framework to extract syllables from ten bird species ranging in their phylogenetic relatedness from 1 to 85 million years, to compare how phylogenetic relatedness influences accuracy. We also evaluate the utility of applying trained models to novel species. Our results indicate that model performance is best on conspecifics, with accuracy progressively decreasing as phylogenetic distance increases between taxa. However, we also find that the application of models trained on multiple distantly related species can improve the overall accuracy to levels near that of training and analyzing a model on the same species. When planning big-data bioacoustics studies, care must be taken in sample design to maximize sample size and minimize human labor without sacrificing accuracy.},
	language = {en},
	number = {12},
	urldate = {2023-03-09},
	journal = {PLOS ONE},
	author = {Provost, Kaiya L. and Yang, Jiaying and Carstens, Bryan C.},
	month = dec,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Vocalization, Birds, Bird song, Syllables, Machine learning, Bioacoustics, Animal phylogenetics, Machine learning algorithms},
	pages = {e0278522},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/JM8VWYDR/Provost et al. - 2022 - The impacts of fine-tuning, phylogenetic distance,.pdf:application/pdf},
}

@article{cohen_automated_2022,
	title = {Automated annotation of birdsong with a neural network that segments spectrograms},
	volume = {11},
	journal = {Elife},
	author = {Cohen, Yarden and Nicholson, David Aaron and Sanchioni, Alexa and Mallaber, Emily K and Skidanova, Viktoriya and Gardner, Timothy J},
	year = {2022},
	note = {Publisher: eLife Sciences Publications Limited},
	pages = {e63853},
}

@misc{nicholson_crowsetta_2022,
	title = {crowsetta},
	url = {https://github.com/NickleDave/crowsetta},
	author = {Nicholson, David},
	month = mar,
	year = {2022},
}

@misc{cohen_tweetynet_2023,
	title = {tweetynet},
	url = {https://doi.org/10.5281/zenodo.7627197},
	publisher = {Zenodo},
	author = {Cohen, Yarden and Nicholson, David},
	month = feb,
	year = {2023},
	doi = {10.5281/zenodo.7627197},
}

@misc{nicholson_vak_2022,
	title = {vak},
	url = {https://doi.org/10.5281/zenodo.6808839},
	publisher = {Zenodo},
	author = {Nicholson, David and Cohen, Yarden},
	month = mar,
	year = {2022},
	doi = {10.5281/zenodo.6808839},
}

@article{mcfee_pump_nodate,
	title = {{PUMP} {UP} {THE} {JAMS}: {V0}.2 {AND} {BEYOND}},
	abstract = {This document describes the changes to the JSON Annotated Music Speciﬁcation (JAMS) format and implementation between v0.1 and v0.2.},
	language = {en},
	author = {McFee, Brian and Humphrey, Eric J and Nieto, Oriol and Salamon, Justin and Bittner, Rachel and Forsyth, Jon and Bello, Juan P},
	pages = {8},
	file = {McFee et al. - PUMP UP THE JAMS V0.2 AND BEYOND.pdf:/home/pimienta/Zotero/storage/73QGQUHQ/McFee et al. - PUMP UP THE JAMS V0.2 AND BEYOND.pdf:application/pdf},
}

@article{humphrey_jams_2014,
	title = {{JAMS}: {A} {JSON} {ANNOTATED} {MUSIC} {SPECIFICATION} {FOR} {REPRODUCIBLE} {MIR} {RESEARCH}},
	abstract = {The continued growth of MIR is motivating more complex annotation data, consisting of richer information, multiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of addressing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, comprehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we provide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and discuss how now is a crucial time to make a concerted effort toward sustainable annotation standards.},
	language = {en},
	author = {Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Forsyth, Jon and Bittner, Rachel M and Bello, Juan P},
	year = {2014},
	pages = {6},
	file = {Humphrey et al. - 2014 - JAMS A JSON ANNOTATED MUSIC SPECIFICATION FOR REP.pdf:/home/pimienta/Zotero/storage/9TY2D667/Humphrey et al. - 2014 - JAMS A JSON ANNOTATED MUSIC SPECIFICATION FOR REP.pdf:application/pdf},
}

@article{roch_tethys_nodate,
	title = {Tethys: {A} workbench and database for passive acoustic metadata},
	abstract = {This project proposes a community standard for the representation of passive acoustic metadata along with a freely available software implementation. Our target audience is the marine mammal community, but the concepts are general and are applicable to a wide variety of taxa. In addition, we address the need to analyze acoustic metadata in the context of other environmental and biological parameters. The implementation provides interfaces to access a wide variety of data available from external services, such solar and lunar rise/set times, sea surface temperature, chlorophyll A, etc., thus permitting extensive data exploration in a workbench environment.},
	language = {en},
	author = {Roch, Marie A and Baumann-Pickering, Simone and Batchelor, Heidi and Širovi, Ana and Berchok, Catherine L and Cholewiak, Danielle and Oleson, Erin M and Soldevilla, Melissa S},
	pages = {5},
	file = {Roch et al. - Tethys A workbench and database for passive acous.pdf:/home/pimienta/Zotero/storage/LW2H9CJ5/Roch et al. - Tethys A workbench and database for passive acous.pdf:application/pdf},
}

@article{dragly_experimental_2018,
	title = {Experimental {Directory} {Structure} ({Exdir}): {An} {Alternative} to {HDF5} {Without} {Introducing} a {New} {File} {Format}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {Experimental {Directory} {Structure} ({Exdir})},
	doi = {10.3389/fninf.2018.00016},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00016},
	abstract = {Natural sciences generate an increasing amount of data in a wide range of formats developed by different research groups and commercial companies. At the same time there is a growing desire to share data along with publications in order to enable reproducible research. Open formats have publicly available specifications which facilitate data sharing and reproducible research. Hierarchical Data Format 5 (HDF5) is a popular open format widely used in neuroscience, often as a foundation for other, more specialized formats. However, drawbacks related to HDF5's complex specification have initiated a discussion for an improved replacement. We propose a novel alternative, the Experimental Directory Structure (Exdir), an open specification for data storage in experimental pipelines which amends drawbacks associated with HDF5 while retaining its advantages. HDF5 stores data and metadata in a hierarchy within a complex binary file which, among other things, is not human-readable, not optimal for version control systems, and lacks support for easy access to raw data from external applications. Exdir, on the other hand, uses file system directories to represent the hierarchy, with metadata stored in human-readable YAML files, datasets stored in binary NumPy files, and raw data stored directly in subdirectories. Furthermore, storing data in multiple files makes it easier to track for version control systems. Exdir is not a file format in itself, but a specification for organizing files in a directory structure. Exdir uses the same abstractions as HDF5 and is compatible with the HDF5 Abstract Data Model. Several research groups are already using data stored in a directory hierarchy as an alternative to HDF5, but no common standard exists. This complicates and limits the opportunity for data sharing and development of common tools for reading, writing, and analyzing data. Exdir facilitates improved data storage, data sharing, reproducible research, and novel insight from interdisciplinary collaboration. With the publication of Exdir, we invite the scientific community to join the development to create an open specification that will serve as many needs as possible and as a foundation for open access to and exchange of data.},
	urldate = {2023-03-09},
	journal = {Frontiers in Neuroinformatics},
	author = {Dragly, Svenn-Arne and Hobbi Mobarhan, Milad and Lepperød, Mikkel E. and Tennøe, Simen and Fyhn, Marianne and Hafting, Torkel and Malthe-Sørenssen, Anders},
	year = {2018},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/TVYNSV9Y/Dragly et al. - 2018 - Experimental Directory Structure (Exdir) An Alter.pdf:application/pdf},
}

@article{cohen_recent_2022,
	title = {Recent {Advances} at the {Interface} of {Neuroscience} and {Artificial} {Neural} {Networks}},
	volume = {42},
	copyright = {Copyright © 2022 the authors. SfN exclusive license.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/42/45/8514},
	doi = {10.1523/JNEUROSCI.1503-22.2022},
	abstract = {Biological neural networks adapt and learn in diverse behavioral contexts. Artificial neural networks (ANNs) have exploited biological properties to solve complex problems. However, despite their effectiveness for specific tasks, ANNs are yet to realize the flexibility and adaptability of biological cognition. This review highlights recent advances in computational and experimental research to advance our understanding of biological and artificial intelligence. In particular, we discuss critical mechanisms from the cellular, systems, and cognitive neuroscience fields that have contributed to refining the architecture and training algorithms of ANNs. Additionally, we discuss how recent work used ANNs to understand complex neuronal correlates of cognition and to process high throughput behavioral data.},
	language = {en},
	number = {45},
	urldate = {2023-03-09},
	journal = {Journal of Neuroscience},
	author = {Cohen, Yarden and Engel, Tatiana A. and Langdon, Christopher and Lindsay, Grace W. and Ott, Torben and Peters, Megan A. K. and Shine, James M. and Breton-Provencher, Vincent and Ramaswamy, Srikanth},
	month = nov,
	year = {2022},
	pmid = {36351830},
	note = {Publisher: Society for Neuroscience
Section: Symposia},
	keywords = {vision, plasticity, behavior, cognition, artificial neural networks, neuromodulators},
	pages = {8514--8523},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/BAMXNLGP/Cohen et al. - 2022 - Recent Advances at the Interface of Neuroscience a.pdf:application/pdf},
}

@misc{baskauf_tdwgac_2022,
	title = {tdwg/ac: {Audubon} {Core} standard 2022-02-23 version},
	url = {https://doi.org/10.5281/zenodo.6590205},
	publisher = {Zenodo},
	author = {Baskauf, Steve and Desmet, Peter and Klazenga, Niels and Blum, Stan and Baker, Ed and Morris, Bob and Webbink, Kate and {danstowell} and Döring, Markus and Junior, Meshack},
	month = may,
	year = {2022},
	doi = {10.5281/zenodo.6590205},
}

@article{recalde_pykanto_nodate,
	title = {pykanto: a python library to accelerate research on wild bird song},
	abstract = {Studying the vocalisations of wild animals can be a challenge due to the limitations of traditional computational methods, which often are time-consuming and lack reproducibility. Here, I present pykanto, a new software package that provides a set of tools to build, manage, and explore large sound databases. It can automatically find discrete units in animal vocalisations, perform semi-supervised labelling of individual repertoires with a new interactive web app, and feed data to deep learning models to study things like individual signatures and acoustic similarity between individuals and populations. To demonstrate its capabilities, I put the library to the test on the vocalisations of male great tits in Wytham Woods, near Oxford, UK. The results show that the identities of individual birds can be accurately determined from their songs and that the use of pykanto improves the efficiency and reproducibility of the process.},
	language = {en},
	author = {Recalde, Nilo Merino},
	file = {Recalde - pykanto a python library to accelerate research o.pdf:/home/pimienta/Zotero/storage/3Y8BVTBP/Recalde - pykanto a python library to accelerate research o.pdf:application/pdf},
}

@article{jadoul_introducing_2018,
	title = {Introducing {Parselmouth}: {A} {Python} interface to {Praat}},
	volume = {71},
	issn = {00954470},
	shorttitle = {Introducing {Parselmouth}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447017301389},
	doi = {10.1016/j.wocn.2018.07.001},
	abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an eﬃcient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Speciﬁcally, we focus on applications in data visualisation, ﬁle manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
	language = {en},
	urldate = {2023-03-09},
	journal = {Journal of Phonetics},
	author = {Jadoul, Yannick and Thompson, Bill and de Boer, Bart},
	month = nov,
	year = {2018},
	pages = {1--15},
	file = {Jadoul et al. - 2018 - Introducing Parselmouth A Python interface to Pra.pdf:/home/pimienta/Zotero/storage/377ZWXW2/Jadoul et al. - 2018 - Introducing Parselmouth A Python interface to Pra.pdf:application/pdf},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {Abstract
            There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2023-03-09},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	pages = {160018},
	file = {Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:/home/pimienta/Zotero/storage/QXZUS324/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:application/pdf},
}

@article{coffey_deepsqueak_2019,
	title = {{DeepSqueak}: a deep learning-based system for detection and analysis of ultrasonic vocalizations},
	volume = {44},
	issn = {0893-133X, 1740-634X},
	shorttitle = {{DeepSqueak}},
	url = {https://www.nature.com/articles/s41386-018-0303-6},
	doi = {10.1038/s41386-018-0303-6},
	language = {en},
	number = {5},
	urldate = {2023-03-09},
	journal = {Neuropsychopharmacology},
	author = {Coffey, Kevin R. and Marx, Ruby E. and Neumaier, John F.},
	month = apr,
	year = {2019},
	pages = {859--868},
	file = {Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf:/home/pimienta/Zotero/storage/E9TXEBJK/Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf:application/pdf},
}

@article{sainburg_finding_2020,
	title = {Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008228},
	doi = {10.1371/journal.pcbi.1008228},
	abstract = {Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species’ vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present a set of computational methods for projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from the spectrograms of vocal signals. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates. Latent projections uncover complex features of data in visually intuitive and quantifiable ways, enabling high-powered comparative analyses of vocal acoustics. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication.},
	language = {en},
	number = {10},
	urldate = {2023-03-09},
	journal = {PLOS Computational Biology},
	author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
	month = oct,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Vocalization, Speech, Finches, Birds, Hidden Markov models, Syllables, Animal communication, Bioacoustics},
	pages = {e1008228},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/B5AANVYI/Sainburg et al. - 2020 - Finding, visualizing, and quantifying latent struc.pdf:application/pdf},
}

@article{sainburg_parallels_2019,
	title = {Parallels in the sequential organization of birdsong and human speech},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-11605-y},
	doi = {10.1038/s41467-019-11605-y},
	abstract = {Human speech possesses a rich hierarchical structure that allows for meaning to be altered by words spaced far apart in time. Conversely, the sequential structure of nonhuman communication is thought to follow non-hierarchical Markovian dynamics operating over only short distances. Here, we show that human speech and birdsong share a similar sequential structure indicative of both hierarchical and Markovian organization. We analyze the sequential dynamics of song from multiple songbird species and speech from multiple languages by modeling the information content of signals as a function of the sequential distance between vocal elements. Across short sequence-distances, an exponential decay dominates the information in speech and birdsong, consistent with underlying Markovian processes. At longer sequence-distances, the decay in information follows a power law, consistent with underlying hierarchical processes. Thus, the sequential organization of acoustic elements in two learned vocal communication signals (speech and birdsong) shows functionally equivalent dynamics, governed by similar processes.},
	language = {en},
	number = {1},
	urldate = {2023-03-09},
	journal = {Nature Communications},
	author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q.},
	month = aug,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Evolution, Psychology, Dynamical systems},
	pages = {3636},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/YY64LADN/Sainburg et al. - 2019 - Parallels in the sequential organization of birdso.pdf:application/pdf},
}

@article{steinfath_fast_2021,
	title = {Fast and accurate annotation of acoustic signals with deep neural networks},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.68837},
	doi = {10.7554/eLife.68837},
	abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast. We here introduce DeepAudioSegmenter (DAS), a method that annotates acoustic signals across species based on a deep-learning derived hierarchical presentation of sound. We demonstrate the accuracy, robustness, and speed of DAS using acoustic signals with diverse characteristics from insects, birds, and mammals. DAS comes with a graphical user interface for annotating song, training the network, and for generating and proofreading annotations. The method can be trained to annotate signals from new species with little manual annotation and can be combined with unsupervised methods to discover novel signal types. DAS annotates song with high throughput and low latency for experimental interventions in realtime. Overall, DAS is a universal, versatile, and accessible tool for annotating acoustic communication signals.},
	urldate = {2023-03-09},
	journal = {eLife},
	author = {Steinfath, Elsa and Palacios-Muñoz, Adrian and Rottschäfer, Julian R and Yuezak, Deniz and Clemens, Jan},
	editor = {Calabrese, Ronald L and Egnor, SE Roian and Troyer, Todd},
	month = nov,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {bird, song, deep learning, acoustic communication, annotation, fly},
	pages = {e68837},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/5BMYZ5SB/Steinfath et al. - 2021 - Fast and accurate annotation of acoustic signals w.pdf:application/pdf},
}

@misc{araya-salas_rraven_2020,
	title = {Rraven: connecting {R} and {Raven} bioacoustic software. {R} package version 1.0.9.},
	author = {Araya-Salas, M.},
	year = {2020},
}

@article{jadoul_introducing_2018-1,
	title = {Introducing {Parselmouth}: {A} {Python} interface to {Praat}},
	volume = {71},
	issn = {00954470},
	shorttitle = {Introducing {Parselmouth}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447017301389},
	doi = {10.1016/j.wocn.2018.07.001},
	abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an eﬃcient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Speciﬁcally, we focus on applications in data visualisation, ﬁle manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
	language = {en},
	urldate = {2023-03-09},
	journal = {Journal of Phonetics},
	author = {Jadoul, Yannick and Thompson, Bill and de Boer, Bart},
	month = nov,
	year = {2018},
	pages = {1--15},
	file = {Jadoul et al. - 2018 - Introducing Parselmouth A Python interface to Pra.pdf:/home/pimienta/Zotero/storage/VWGPSVQB/Jadoul et al. - 2018 - Introducing Parselmouth A Python interface to Pra.pdf:application/pdf},
}

@misc{haupert_scikit-maadscikit-maad_2022,
	title = {scikit-maad/scikit-maad: {Stable} {Release} : v1.3.12},
	url = {https://doi.org/10.5281/zenodo.7324324},
	publisher = {Zenodo},
	author = {HAUPERT, Sylvain and Ulloa, Juan Sebastian and Gil, Juan Felipe Latorre and {scikit-maad} and Suarez, Gabriel Alejandro Perilla},
	month = nov,
	year = {2022},
	doi = {10.5281/zenodo.7324324},
}

@article{buschmeier_textgridtools_nodate,
	title = {{TEXTGRIDTOOLS}: {A} {TEXTGRID} {PROCESSING} {AND} {ANALYSIS} {TOOLKIT} {FOR} {PYTHON}},
	abstract = {In this paper we present TEXTGRIDTOOLS, a free Python package for processing, querying and manipulating Praat’s TextGrid ﬁles. TEXTGRIDTOOLS improves on many deﬁciencies of Praat’s embedded scripting language by providing a clean data model for TextGrid objects and their attributes, and offering functionality for common annotation-related tasks, for instance calculation of interannotator agreement measures. Owing to seamless integration with other Python tools, such as data analysis libraries and interactive interpreters, users gain access to a versatile and powerful computing environment without the need of repeated format conversions.},
	language = {en},
	author = {Buschmeier, Hendrik and Włodarczak, Marcin},
	file = {Buschmeier and Włodarczak - TEXTGRIDTOOLS A TEXTGRID PROCESSING AND ANALYSIS .pdf:/home/pimienta/Zotero/storage/SM756RKV/Buschmeier and Włodarczak - TEXTGRIDTOOLS A TEXTGRID PROCESSING AND ANALYSIS .pdf:application/pdf},
}

@article{berman_measuring_2018,
	title = {Measuring behavior across scales},
	volume = {16},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-018-0494-7},
	doi = {10.1186/s12915-018-0494-7},
	abstract = {The need for high-throughput, precise, and meaningful methods for measuring behavior has been amplified by our recent successes in measuring and manipulating neural circuitry. The largest challenges associated with moving in this direction, however, are not technical but are instead conceptual: what numbers should one put on the movements an animal is performing (or not performing)? In this review, I will describe how theoretical and data analytical ideas are interfacing with recently-developed computational and experimental methodologies to answer these questions across a variety of contexts, length scales, and time scales. I will attempt to highlight commonalities between approaches and areas where further advances are necessary to place behavior on the same quantitative footing as other scientific fields.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {BMC Biology},
	author = {Berman, Gordon J.},
	month = feb,
	year = {2018},
	keywords = {Autoregressive Hidden Markov Models (AR-HMM), Behavioral Representation, Computational Psychiatry, Dynamic Mode Decomposition, Lagrangian Coherent Structures},
	pages = {23},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/2E42N9L8/Berman - 2018 - Measuring behavior across scales.pdf:application/pdf},
}

@article{pereira_quantifying_2020,
	title = {Quantifying behavior to understand the brain},
	volume = {23},
	copyright = {2020 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-020-00734-z},
	doi = {10.1038/s41593-020-00734-z},
	abstract = {Over the past years, numerous methods have emerged to automate the quantification of animal behavior at a resolution not previously imaginable. This has opened up a new field of computational ethology and will, in the near future, make it possible to quantify in near completeness what an animal is doing as it navigates its environment. The importance of improving the techniques with which we characterize behavior is reflected in the emerging recognition that understanding behavior is an essential (or even prerequisite) step to pursuing neuroscience questions. The use of these methods, however, is not limited to studying behavior in the wild or in strictly ethological settings. Modern tools for behavioral quantification can be applied to the full gamut of approaches that have historically been used to link brain to behavior, from psychophysics to cognitive tasks, augmenting those measurements with rich descriptions of how animals navigate those tasks. Here we review recent technical advances in quantifying behavior, particularly in methods for tracking animal motion and characterizing the structure of those dynamics. We discuss open challenges that remain for behavioral quantification and highlight promising future directions, with a strong emphasis on emerging approaches in deep learning, the core technology that has enabled the markedly rapid pace of progress of this field. We then discuss how quantitative descriptions of behavior can be leveraged to connect brain activity with animal movements, with the ultimate goal of resolving the relationship between neural circuits, cognitive processes and behavior.},
	language = {en},
	number = {12},
	urldate = {2023-04-04},
	journal = {Nature Neuroscience},
	author = {Pereira, Talmo D. and Shaevitz, Joshua W. and Murthy, Mala},
	month = dec,
	year = {2020},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Sensorimotor processing},
	pages = {1537--1549},
	file = {Full Text PDF:/home/pimienta/Zotero/storage/479ZYLH6/Pereira et al. - 2020 - Quantifying behavior to understand the brain.pdf:application/pdf},
}

@misc{nilo_nilomrpykanto_2023,
	title = {nilomr/pykanto: v0.1.4},
	shorttitle = {nilomr/pykanto},
	url = {https://zenodo.org/record/7734979},
	abstract = {A python library for animal vocalisation analysis},
	urldate = {2023-04-04},
	publisher = {Zenodo},
	author = {Nilo},
	month = mar,
	year = {2023},
	doi = {10.5281/zenodo.7734979},
}

@misc{recalde_pykanto_2023,
	title = {pykanto: a python library to accelerate research on wild bird song},
	shorttitle = {pykanto},
	url = {http://arxiv.org/abs/2302.10340},
	doi = {10.48550/arXiv.2302.10340},
	abstract = {Studying the vocalisations of wild animals can be a challenge due to the limitations of traditional computational methods, which often are time-consuming and lack reproducibility. Here, I present pykanto, a new software package that provides a set of tools to build, manage, and explore large sound databases. It can automatically find discrete units in animal vocalisations, perform semi-supervised labelling of individual repertoires with a new interactive web app, and feed data to deep learning models to study things like individual signatures and acoustic similarity between individuals and populations. To demonstrate its capabilities, I put the library to the test on the vocalisations of male great tits in Wytham Woods, near Oxford, UK. The results show that the identities of individual birds can be accurately determined from their songs and that the use of pykanto improves the efficiency and reproducibility of the process.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Recalde, Nilo Merino},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10340 [cs, eess, q-bio]},
	keywords = {Computer Science - Sound, D.2.13, D.2.2, Electrical Engineering and Systems Science - Audio and Speech Processing, J.3, Quantitative Biology - Populations and Evolution, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/pimienta/Zotero/storage/XFY6BNVL/Recalde - 2023 - pykanto a python library to accelerate research o.pdf:application/pdf;arXiv.org Snapshot:/home/pimienta/Zotero/storage/8V8BA6ZG/2302.html:text/html},
}


@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@InProceedings{ mckinney-proc-scipy-2010,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}
